/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package com.unsafepointer.word_count

import kotlinx.cli.ArgParser
import kotlinx.cli.ArgType
import kotlinx.cli.default
import org.apache.log4j.Logger
import org.apache.log4j.varia.NullAppender
import org.apache.spark.SparkFiles
import org.apache.spark.api.java.JavaSparkContext
import org.apache.spark.api.java.function.FlatMapFunction
import org.apache.spark.sql.SparkSession
import org.jetbrains.kotlinx.spark.api.toPairRDD
import org.jetbrains.kotlinx.spark.api.tuples.component1
import org.jetbrains.kotlinx.spark.api.tuples.component2
import scala.Tuple2
import java.net.URI

class FlattenLineWords: FlatMapFunction<String, String> {
    override fun call(line: String): MutableIterator<String> {
        if (line.isEmpty()) {
            return emptyList<String>().toMutableList().iterator()
        }
        val words = line.replace("[^a-zA-Z ]".toRegex(), "").lowercase().split("\\s+".toRegex())
        return words.toMutableList().iterator()
    }
}

fun main(args: Array<String>) {
    val parser = ArgParser("word-counter")
    val bookUrl by parser.option(ArgType.String, shortName = "u", description = "Text file URL").default("https://www.gutenberg.org/files/2600/2600-0.txt")
    val verbose by parser.option(ArgType.Boolean, shortName = "v", description = "Verbose mode, display Spark logs").default(false)

    parser.parse(args)

    if (!verbose) {
        Logger.getRootLogger().removeAllAppenders()
        Logger.getRootLogger().addAppender(NullAppender())
    }

    val bookUri = URI(bookUrl)
    val fileName = bookUri.path.split("/").last()
    val sparkSession = SparkSession.builder().master("local[2]").appName("WordCounter").orCreate
    val sparkContext = JavaSparkContext(sparkSession.sparkContext())

    sparkContext.addFile(bookUrl)
    val sparkFile = SparkFiles.get(fileName)
    val bookRDD = sparkContext.textFile("file://$sparkFile", 1)

    val wordsRDD = bookRDD.flatMap(FlattenLineWords())
    val countRDD = wordsRDD.mapToPair { word -> Tuple2(word, 1) }
    val reducedCountRDD = countRDD.reduceByKey { x, y -> x + y }
    // Workaround to secondary sort not yet supported: https://issues.apache.org/jira/browse/SPARK-3655
    val result = reducedCountRDD.map { it.swap() }.toPairRDD().sortByKey(false)
    val uniqueWords = result.count()
    val mostCommonWords = result.take(5).joinToString { tuple ->
        val (count, word) = tuple
        "$word ($count)"
    }

    println("Number of unique words: $uniqueWords")
    println("Most common words: $mostCommonWords")

    sparkSession.stop()
}
